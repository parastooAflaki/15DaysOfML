{"cells":[{"metadata":{},"cell_type":"markdown","source":"Handling Categorized data\n---\n\n* use OrdinalEncoder class\n\n            from Sklearn.preprocessing import OrdinalEncoder\n            ordinal_encoder =  OrdinalEncoder()\n            dataset_encoded = ordinal_encoder.fit_transform(dataset)\n            ordinal_encoder.categories_\n* use One-hot Encoding\n\n            from Sklearn.preprocessing import OneHotEncoder\n            cat_encoder = OneHotEncoder()\n            dataset_encoded = cat_encoder.fit_transform(dataset)\n            cat_encoder.categories_\n   * Notice that the output is a SciPy sparse matrix, instead of a NumPy array so instead a sparse matrix only stores the location of the nonzero elements. \n   You can use it mostly like a normal 2D array,21 but if you really want to convert it to a (dense) NumPy array, just call the **toarray()** method\n            \n            \n* use Embedding            \n  If a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance.       \n   Alternatively, you could replace each category with a learnable, low-dimensional vector called an embedding.\n   \n   \n   Custom Transformer\n   ---\n   You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods: **fit() (returning self)**, **transform()**, and **fit_transform()**."},{"metadata":{},"cell_type":"markdown","source":"Feature Scaling\n---\n* min-max method :\n\n    Simply shift values so they endup being in range 0 to 1. \n              \n        from sklearn MinMaxScaler, \n    \n        *Feature_range* hyperparameter which lets you change the range.\n* Standardization \n    \n    Substract by mean. Divide by svd -> result has mean 0 and variance 1.\n    \n    ** standardization does not bound values to a specific range, which may be a problem for some algorithms. but is less affected by outliers.\n            \n         from sklearn StandardScalar\n    ---     \n\n### **Fit any transformer only on training data then transform training/test data**      \n\n---\nPipeline\n---\nScikit-Learn provides the Pipeline class to help with such sequences of transformations.\n        \n        from sklearn.pipeline import Pipeline\n        num_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n          ])\n* **ColumnTransformer** It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column.\n            \n        from sklearn.compose import ColumnTransformer\n        ull_pipeline = ColumnTransformer([\n        (\"name\", transformer_func | \"passthrough\" | \"drop\", list of columns),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n        ])\nThis applies each transformer to the appropriate columns and concatenates the outputs along the second axis.\nIf the output of one transformer is sparse matrix while the other one is dense matrix, the ColumnTransformer estimates the density of the final matrix, (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by\ndefault, sparse_threshold=0.3).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\n\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\n\nfrom sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\n\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forests** work by training many Decision Trees on random subsets of\nthe features, then averaging out their predictions. \nBuilding a model on top of many other models is called **Ensemble Learning**, and it is often a great way to push ML algorithms\neven further.\n\n\nThe main ways to fix underfitting are to select a more\npowerful model, to feed the training algorithm with better features, or to reduce the\nconstraints on the model.\n\nPossible solutions for overfitting are\nto simplify the model, constrain it (i.e., regularize it), or get a lot more training data."},{"metadata":{},"cell_type":"markdown","source":"Better Evaluation Using Cross-Validation\n---\n\nScikit-Learn’s K-fold cross-validation: randomly splits the training set into 10 distinct subsets called folds, then it\ntrains and evaluates the Decision Tree model 10 times, picking a different fold for\nevaluation every time and training on the other 9 folds. The result is an array containing\nthe 10 evaluation scores:"},{"metadata":{},"cell_type":"markdown","source":"# Notice!!\nScikit-Learn’s cross-validation features expect a utility function\n(greater is better) rather than a cost function (lower is better), so\nthe scoring function is actually the opposite of the MSE (i.e., a negative\nvalue), which is why the preceding code computes -scores\nbefore calculating the square root.\n\n\nCross-validation allows you to get not only an estimate of the performance of your model, but also a measure\nof how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\nscore of approximately 71,407, generally ±2,439. You would not have this information\nif you just used one validation set. But cross-validation comes at the cost of training\nthe model several times, so it is not always possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\nscoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Saving the model\nTo save your sklearn model you can use python's pickle or the **joblib library**, which is more efficient at serializing large NumPy arrays.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\njoblib.dump(my_model, \"my_model.pkl\")\n# and later...\nmy_model_loaded = joblib.load(\"my_model.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine-tune Model\n---\n\n* Grid Search\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}